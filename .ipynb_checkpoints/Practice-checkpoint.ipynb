{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524e4987-57eb-48d2-bccf-f84bb3e6e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/21 09:35:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark Session\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185cf767-012a-48c6-999a-e4526bad50eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vinayaks-mbp.station:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1192d7590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b6bb9-9e9b-45ee-9b8c-d218e03dfa14",
   "metadata": {},
   "source": [
    "##### Tips:\n",
    "\n",
    "1. Spark Prefers Lazy Evaluation Spark will not do the transformation until we call an action. we can call n number of transformations but it will not get executed until the action get called.\n",
    "\n",
    "2. Spark UI is application UI, It helps us understand what is happening with the data in background with the jobs.\n",
    "\n",
    "3. we can also use spark interactive shell to work with spark(on mac, you can use spark-shell command in terminal to use spark shell.)\n",
    "\n",
    "Interview Question:\n",
    "\n",
    "Since  spark session you can assign a different spark sessions.(vinayak = spark.getActiveSession() and after this spark gives memory location where spark session is on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9448dcab-7d3c-4b86-8286-778abedc8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5253199-a599-451a-a17d-2e110fd0ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very Important: You always need a datafram to manipulate columns\n",
    "# Create emp DataFrame\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8684ea47-d83c-44d8-a186-cd983acdfd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show emp dataframe (ACTION)\n",
    "\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa17971-199f-42dc-9a59-714e2727bdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('employee_id', StringType(), True), StructField('department_id', StringType(), True), StructField('name', StringType(), True), StructField('age', StringType(), True), StructField('gender', StringType(), True), StructField('salary', StringType(), True), StructField('hire_date', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema for emp\n",
    "\n",
    "emp.schema\n",
    "\n",
    "# Spark Stores the schema basicaly in StructType along with each coloumn as StructField(\"Coloumn Name, DataType, and Nullable Field\"). \n",
    "# Nullable Field Explaination: In the StructType definition for a PySpark DataFrame schema, the True and False values are used to specify whether each field is nullable or not.\n",
    "# you can edit this to specify The True after StringType() in each StructField indicates that the corresponding field is nullable, meaning it can have null values.\n",
    "# If you set this value to False, it means that the field is not nullable and cannot contain null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db21a35b-15f3-4fb8-87ab-49b12e359f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Example for Schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema_string = \"name string, age int\"\n",
    "\n",
    "schema_spark =  StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "# This is a spark Schema you can use above schema example, if you don't want to use this spark schema you can give spark_string to pyspark and then spark will convert it into \n",
    "# it's spark schema automatically. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9bc2d7-83a1-40ae-a6ab-5aa40fae3cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'salary'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns and expression\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "\n",
    "# You can use col('salary')/ expr('salary') we will get the same result. because, any manipulation done on column is considered as a expression to call a column from dataframe\n",
    "# we can use emp[\"salary\"]\n",
    "emp[\"salary\"]\n",
    "emp.salary\n",
    "# expr['salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42bd2208-4545-4ca1-b100-2e8cf1b51a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT columns\n",
    "# select employee_id, name, age, salary from emp\n",
    "\n",
    "emp_filtered = emp.select(col(\"employee_id\"), expr(\"name\"), emp.age, emp.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dd50fef-6e96-4eed-b49a-39975f782444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+\n",
      "|employee_id|         name|age|salary|\n",
      "+-----------+-------------+---+------+\n",
      "|        001|     John Doe| 30| 50000|\n",
      "|        002|   Jane Smith| 25| 45000|\n",
      "|        003|    Bob Brown| 35| 55000|\n",
      "|        004|    Alice Lee| 28| 48000|\n",
      "|        005|    Jack Chan| 40| 60000|\n",
      "|        006|    Jill Wong| 32| 52000|\n",
      "|        007|James Johnson| 42| 70000|\n",
      "|        008|     Kate Kim| 29| 51000|\n",
      "|        009|      Tom Tan| 33| 58000|\n",
      "|        010|     Lisa Lee| 27| 47000|\n",
      "|        011|   David Park| 38| 65000|\n",
      "|        012|   Susan Chen| 31| 54000|\n",
      "|        013|    Brian Kim| 45| 75000|\n",
      "|        014|    Emily Lee| 26| 46000|\n",
      "|        015|  Michael Lee| 37| 63000|\n",
      "|        016|  Kelly Zhang| 30| 49000|\n",
      "|        017|  George Wang| 34| 57000|\n",
      "|        018|    Nancy Liu| 29| 50000|\n",
      "|        019|  Steven Chen| 36| 62000|\n",
      "|        020|    Grace Kim| 32| 53000|\n",
      "+-----------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW Dataframe (ACTION)\n",
    "\n",
    "emp_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1fddd5-1a4e-47d3-a98c-08bb9e6a166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using expr for select\n",
    "# select employee_id as emp_id, name, cast(age as int) as age, salary from emp_filtered\n",
    "\n",
    "emp_casted = emp_filtered.select(expr(\"employee_id as emp_id\"), emp.name, expr(\"cast(age as int) as age\"), emp.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "419f9850-2ef3-43b5-82e2-5af228f3c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   001|     John Doe| 30| 50000|\n",
      "|   002|   Jane Smith| 25| 45000|\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   004|    Alice Lee| 28| 48000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   008|     Kate Kim| 29| 51000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   010|     Lisa Lee| 27| 47000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   014|    Emily Lee| 26| 46000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   016|  Kelly Zhang| 30| 49000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   018|    Nancy Liu| 29| 50000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW Dataframe (ACTION)\n",
    "\n",
    "emp_casted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "630452d5-5280-4f2f-be9c-725aca9018a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_casted_1 = emp_filtered.selectExpr(\"employee_id as emp_id\", \"name\", \"cast(age as int) as age\", \"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7813ba-6810-4047-9527-6f849bcdb3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   001|     John Doe| 30| 50000|\n",
      "|   002|   Jane Smith| 25| 45000|\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   004|    Alice Lee| 28| 48000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   008|     Kate Kim| 29| 51000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   010|     Lisa Lee| 27| 47000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   014|    Emily Lee| 26| 46000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   016|  Kelly Zhang| 30| 49000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   018|    Nancy Liu| 29| 50000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa6b3812-f3d4-4ad4-b68d-aeeb62959fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc35136-6829-460d-b61b-6ac017cde1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter emp based on Age > 30\n",
    "# select emp_id, name, age, salary from emp_casted where age > 30\n",
    "\n",
    "emp_final = emp_casted.select(\"emp_id\", \"name\", \"age\", \"salary\").where(\"age > 30\")\n",
    "\n",
    "# Don't worry if you found this little hard. You can directly use SQL schemas in the pyspark there is an example next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dab35d2-f1e5-48ad-8efd-5193084620d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Why do we have to create a Tempview in pyspark to use sql queries?\n",
    "# createOrReplaceTempView: is essential for SQL queries: It registers the DataFrame as a view that SQL context can interact with.\n",
    "# Temporary views make DataFrames queryable by SQL: This step bridges the gap between DataFrame operations and SQL queries in PySpark.\n",
    "emp_casted.createOrReplaceTempView(\"emp_view\")\n",
    "emp_final = spark.sql(\"SELECT emp_id, name, age, salary FROM emp_view WHERE age > 30\")\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29b15458-15b8-48eb-b4bf-77c6b9f4f13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|   003|    Bob Brown| 35| 55000|\n",
      "|   005|    Jack Chan| 40| 60000|\n",
      "|   006|    Jill Wong| 32| 52000|\n",
      "|   007|James Johnson| 42| 70000|\n",
      "|   009|      Tom Tan| 33| 58000|\n",
      "|   011|   David Park| 38| 65000|\n",
      "|   012|   Susan Chen| 31| 54000|\n",
      "|   013|    Brian Kim| 45| 75000|\n",
      "|   015|  Michael Lee| 37| 63000|\n",
      "|   017|  George Wang| 34| 57000|\n",
      "|   019|  Steven Chen| 36| 62000|\n",
      "|   020|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW Dataframe (ACTION)\n",
    "\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54817dfb-13b7-41a5-9581-a4da1361c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data back as CSV (ACTION)\n",
    "\n",
    "emp_final.write.format(\"csv\").save(\"data/output/2/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24192b11-b2d1-49e9-ab29-5f9e69a09928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('emp_id', IntegerType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('salary', IntegerType(), True)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonus TIP\n",
    "\n",
    "schema_str = \"emp_id int,name string, age int, salary int\"\n",
    "\n",
    "from pyspark.sql.types import _parse_datatype_string\n",
    "\n",
    "schema_spark = _parse_datatype_string(schema_str)\n",
    "\n",
    "schema_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09106bb-af5a-4c5d-a26c-cc99343b09c8",
   "metadata": {},
   "source": [
    "Use Cases for Defining and Parsing Schema Strings\n",
    "Dynamic Schema Definition:\n",
    "\n",
    "When working with data that has a schema that can change or is defined at runtime, using a schema string allows for flexibility. You can easily modify the schema definition as a string, which is simpler and more readable than manually constructing StructType and StructField objects.\n",
    "Configuration-Driven Schema:\n",
    "\n",
    "In applications where schemas are stored in configuration files (e.g., JSON, YAML), itâ€™s convenient to define schemas as strings. You can read these schema strings from the configuration file and parse them into PySpark schema objects. This approach separates schema definitions from code, making it easier to manage and update schemas without modifying the codebase.\n",
    "Code Readability and Maintainability:\n",
    "\n",
    "Using schema strings improves the readability and maintainability of your code. Defining a schema in a concise string format is more straightforward than the verbose StructType and StructField definitions. This is especially useful for complex schemas with many fields.\n",
    "Interoperability with Other Tools:\n",
    "\n",
    "When integrating PySpark with other tools and platforms that define schemas in a string format (e.g., SQL databases, schema registries), using schema strings in PySpark allows for smoother interoperability. You can directly use these schema definitions in PySpark without conversion overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6af7cd-079b-4def-b56b-c7d2c72414cc",
   "metadata": {},
   "source": [
    "## Spark Hands-on\n",
    "\n",
    "1. Adding Columns\n",
    "2. Using Literals/Static values\n",
    "3. Renaming Columns\n",
    "4. Removing Columns\n",
    "5. Filtering and LIMIT for DataFrame\n",
    "6. Structured Transformations -withColumn, withcolumnRenamed, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637c0e2-c5d8-4cbf-b9ed-a2179abc8e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
